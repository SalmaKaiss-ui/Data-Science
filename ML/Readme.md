# COURS DE SCIENCE DES DONN√âES
## √âcole Nationale de Commerce et de Gestion (ENCG) - 4√®me Ann√©e
 
 
 ## Salma Kaiss

 
 ## Nom du jeux: Wine Quality

## Descriptif:

Le dataset "Wine Quality" de l‚ÄôUCI Machine Learning Repository contient des informations sur des √©chantillons de vins rouges et blancs de la r√©gion Vinho Verde au Portugal. Il se compose de deux jeux de donn√©es distincts, totalisant 4898 instances pour le vin blanc et 1599 pour le vin rouge. Chaque √©chantillon est d√©crit par 11 variables physico-chimiques continues, telles que l‚Äôacidit√© fixe, l‚Äôacidit√© volatile, l‚Äôacide citrique, le sucre r√©siduel, les chlorures, les niveaux de dioxyde de soufre (libre et total), la densit√©, le pH, les sulfates et la teneur en alcool. La qualit√© du vin, not√©e sur une √©chelle de 0 √† 10, constitue la variable cible obtenue √† partir de donn√©es sensorielles provenant d‚Äôexperts. 

Ce jeu de donn√©es est souvent utilis√© pour cr√©er des mod√®les de classification ou de r√©gression afin de pr√©dire la qualit√© des vins √† partir de leurs caract√©ristiques chimiques. Les classes de qualit√© sont ordonn√©es mais d√©s√©quilibr√©es, ce qui invite √† des approches adapt√©es notamment en gestion des d√©s√©quilibres et en s√©lection de caract√©ristiques. Ce dataset est une r√©f√©rence pour l‚Äôanalyse des donn√©es li√©es √† la viticulture et la mod√©lisation pr√©dictive en ≈ìnologie.


**Codes Python: Installation du package**

```python
pip install ucimlrepo     
```

# üìù Interpr√©tation 

Le code utilise `fetch_ucirepo` pour t√©l√©charger automatiquement le dataset *Wine Quality* depuis l‚ÄôUCI Repository.
Il s√©pare ensuite les donn√©es en deux parties : `X` pour les variables explicatives et `y` pour la variable cible.
Enfin, il affiche les m√©tadonn√©es du dataset ainsi que les informations d√©taill√©es sur chaque variable afin de mieux comprendre sa structure.



```python
from ucimlrepo import fetch_ucirepo 
  
# fetch dataset 
wine_quality = fetch_ucirepo(id=186) 
  
# data (as pandas dataframes) 
X = wine_quality.data.features 
y = wine_quality.data.targets 
  
# metadata 
print(wine_quality.metadata) 
  
# variable information 
print(wine_quality.variables)    
```

# üìù Interpr√©tation 

Le code importe le mod√®le `KNeighborsClassifier`, un algorithme de classification bas√© sur les plus proches voisins.
La commande `?KNeighborsClassifier` permet d‚Äôafficher sa documentation pour comprendre ses param√®tres et son utilisation.



```python
from sklearn.neighbors import KNeighborsClassifier
# to get the online help, type:
?KNeighborsClassifier  
```

# 1 Data analysis

## Load the data and show its summary:x

# üìù Interpr√©tation 

Le code commence par convertir les donn√©es originales du dataset *Wine Quality* en un DataFrame complet, incluant la colonne indiquant la couleur du vin.
Il filtre ensuite uniquement les vins blancs et retire la colonne `color`, puis affiche un r√©sum√© du dataset ainsi que les premi√®res lignes pour visualiser la structure des donn√©es.



```python
import pandas as pd
import numpy as np

# The dataset has already been fetched using ucimlrepo in cell _5Nt2FgKoPiy
# wine_quality = fetch_ucirepo(id=186)

# Create a full DataFrame from the original data, which includes the 'color' column
df = wine_quality.data.original.copy()

# Filter for white wine data, as the original link was for winequality-white.csv
df = df[df['color'] == 'white'].drop(columns=['color'])

print("\n========= Dataset summary ========= \n")
df.info()
print("\n========= A few first samples ========= \n")
print(df.head())
```


##  Form the arrays X ‚àà R^N*d of the input variables and Y‚àà R^N the output. What are the wine qualities and the related number of samples ?

# üìù Interpr√©tation 

Le code s√©pare les donn√©es en pla√ßant toutes les variables explicatives dans `X` en retirant la colonne `quality`, et en mettant la variable cible `quality` dans `Y`.
Il affiche ensuite la distribution des diff√©rentes notes de qualit√© du vin pour montrer combien d‚Äô√©chantillons appartiennent √† chaque cat√©gorie.



```python
 X = df.drop("quality", axis=1) #we drop the column "quality"
 Y = df["quality"]
 print("\n========= Wine Qualities ========= \n")
 print(Y.value_counts())
```

##  To form a binary classification problem, we group the data by quality level.

# üìù Interpr√©tation 

Ce code transforme la variable `Y` en une classification binaire o√π les vins ayant une qualit√© inf√©rieure ou √©gale √† 5 sont √©tiquet√©s comme **mauvais** (`0`), tandis que les autres sont consid√©r√©s comme **bons** (`1`).
Il cr√©e ainsi une nouvelle version simplifi√©e de la cible pour pr√©parer un mod√®le de classification binaire.


```python
 # bad wine (y=0) : quality <= 5 and good quality (y= 1) otherwise
 Y = [0 if val <=5 else 1 for val in Y]
```


##   Perform a statistical analysis (mean, variance, correlation ...) of the input variables.
 Comments on the results.


# üìù Interpr√©tation 

Le code commence par tracer un **boxplot** de toutes les variables explicatives afin de visualiser leur distribution et rep√©rer les valeurs extr√™mes, en orientant les bo√Ætes verticalement et en tournant les √©tiquettes pour faciliter la lecture.
Ensuite, il calcule la **matrice de corr√©lation** des variables et affiche un **heatmap**, ce qui permet d‚Äôidentifier quelles caract√©ristiques sont fortement corr√©l√©es entre elles.


```python
  import matplotlib.pyplot as plt
 import seaborn as sns
 plt.figure()
 ax = plt.gca()
 sns.boxplot(data=X,orient="v",palette="Set1",width=1.5, notch=True)
 ax.set_xticklabels(ax.get_xticklabels(),rotation=90)
 plt.figure()
 corr = X.corr()
 sns.heatmap(corr)
```


<img width="552" height="534" alt="image" src="https://github.com/user-attachments/assets/f320b7a4-788e-4df0-85aa-06b5533c3ed7" />


# üìù Analyse du boxplot

Ce boxplot montre la distribution des diff√©rentes caract√©ristiques chimiques du vin blanc. On observe que **la majorit√© des variables ont des valeurs relativement faibles** et sont regroup√©es pr√®s de z√©ro, tandis que certaines variables se distinguent par des valeurs beaucoup plus √©lev√©es.

* **free_sulfur_dioxide** et **total_sulfur_dioxide** pr√©sentent des valeurs nettement plus grandes que les autres, avec de nombreux **outliers**, ce qui indique une forte variabilit√© et des mesures extr√™mes dans ces deux caract√©ristiques.
* Des variables comme **residual_sugar**, **chlorides** et **volatile_acidity** montrent √©galement quelques valeurs extr√™mes mais dans des proportions plus mod√©r√©es.
* Les variables telles que **pH**, **density**, **alcohol**, **citric_acid**, et **fixed_acidity** ont des distributions plus compactes et moins d‚Äô√©carts inhabituels.



<img width="648" height="540" alt="image" src="https://github.com/user-attachments/assets/1d9c0c5b-e916-43cf-9d76-ca61cfb80f9f" />


# üìù Analyse du heatmap de corr√©lation

Ce heatmap montre les corr√©lations entre toutes les variables chimiques du vin blanc. Les couleurs plus claires indiquent une **corr√©lation positive forte**, tandis que les couleurs fonc√©es indiquent une **corr√©lation n√©gative** ou faible.

### üîç **Points importants :**

* **free_sulfur_dioxide** et **total_sulfur_dioxide** sont fortement corr√©l√©s (zone tr√®s claire), ce qui indique que lorsque l‚Äôun augmente, l‚Äôautre augmente souvent aussi.
* **density** est positivement corr√©l√©e avec **residual_sugar** et **chlorides**, ce qui est logique car plus il y a de sucre ou de sel, plus le vin devient dense.
* **alcohol** est n√©gativement corr√©l√© avec **density**, ce qui signifie que les vins plus alcoolis√©s sont g√©n√©ralement moins denses.
* La plupart des autres corr√©lations sont **faibles** (dominance de couleurs interm√©diaires), sugg√©rant que beaucoup de variables varient ind√©pendamment les unes des autres.
* **pH** montre des relations tr√®s faibles avec la majorit√© des autres variables.



# 2 Classification

## Data Split


# üìù Interpr√©tation 

Le code utilise `train_test_split` pour diviser les donn√©es en trois ensembles : un **ensemble d‚Äôapprentissage**, un **ensemble de validation**, et un **ensemble de test**, tout en conservant la proportion des classes gr√¢ce au param√®tre `stratify`.
D‚Äôabord, il s√©pare un tiers des donn√©es pour le test, puis il divise le reste en deux parts √©gales afin de cr√©er les ensembles d‚Äôapprentissage et de validation.


```python
 from sklearn.model_selection import train_test_split
 Xa, Xt, Ya, Yt = train_test_split(X, Y, shuffle=True, test_size=1/3,
 stratify=Y)
 Xa, Xv, Ya, Yv = train_test_split(Xa, Ya, shuffle=True, test_size=0.5,
 stratify=Ya)
```

## k nearest neighbor (k-NN) classification

# üìù Interpr√©tation 

Le code commence par entra√Æner un mod√®le **KNN** avec `k = 3` sur les donn√©es d‚Äôapprentissage, puis il pr√©dit les √©tiquettes sur l‚Äôensemble de validation et calcule le taux d‚Äôerreur.
Ensuite, une boucle teste plusieurs valeurs de `k` (1, 3, 5, ‚Ä¶, 35), en √©valuant pour chacune l‚Äôerreur d‚Äôapprentissage et l‚Äôerreur de validation, afin d‚Äôidentifier la valeur de `k` qui minimise l‚Äôerreur de validation et constitue ainsi le meilleur choix pour le mod√®le.



```python
 from sklearn.neighbors import KNeighborsClassifier
 # Fit the model on (Xa, Ya)
 k = 3
 clf = KNeighborsClassifier(n_neighbors = k)
 clf.fit(Xa, Ya)      
    # Predict the labels of samples in Xv
 Ypred_v = clf.predict(Xv)
 # evaluate classification error rate
 from sklearn.metrics import accuracy_score
 error_v = 1-accuracy_score(Yv, Ypred_v)
    # some hints
 k_vector = np.arange(1, 37, 2) #define a vector of k=1, 3, 5, ...
 error_train = np.empty(k_vector.shape)
 error_val = np.empty(k_vector.shape)
 for ind, k in enumerate(k_vector):
 #fit with k
 clf = KNeighborsClassifier(n_neighbors = k)
 clf.fit(Xa, Ya)
 # predict and evaluate on training and validation sets
 Ypred_train = ...
 error_train[ind] = ...
 ...
# some hints: get the min error and related k-value
 err_min, ind_opt = error_val.min(), error_val.argmin()
 k_star = k_vector[ind_opt]
```

## Normalize or not normalize the data ?

# üìù Interpr√©tation 

Le code utilise `StandardScaler` pour **normaliser** les donn√©es d‚Äôapprentissage en leur retirant la moyenne et en les divisant par leur √©cart-type, ce qui donne √† chaque variable la m√™me √©chelle.
Apr√®s avoir ajust√© le scaler sur `Xa`, il applique cette transformation √† `Xa` et `Xv`, afin que les donn√©es normalis√©es soient coh√©rentes et pr√™tes pour l‚Äôentra√Ænement d‚Äôun mod√®le sensible aux distances comme le KNN.


```python
 from sklearn.preprocessing import StandardScaler
 sc = StandardScaler(with_mean=True, with_std=True)
 sc = sc.fit(Xa)
 Xa_n = sc.transform(Xa)
 Xv_n = sc.transform(Xv)
```








 

